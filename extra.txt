import tensorflow as tf

# List available devices
devices = tf.config.list_physical_devices()
print("Available devices:", devices)

# Check if a GPU is available
if tf.config.list_physical_devices('GPU'):
    print("GPU is available")
else:
    print("GPU not available")





from tensorflow.keras import models, layers
from tensorflow.keras.applications import MobileNetV2

def create_mobilenet_encoder(input_shape):
    # Load MobileNetV2 with pre-trained ImageNet weights
    base_model = MobileNetV2(include_top=False, input_shape=input_shape, weights='imagenet')
    
    # Extract features at various stages to perform the skip connections later
    layer_names = [
        'block_1_expand_relu',   # 64x64
        'block_3_expand_relu',   # 32x32
        'block_6_expand_relu',   # 16x16
        'block_13_expand_relu',  # 8x8
        'block_16_project',      # 4x4
    ]
    layers = [base_model.get_layer(name).output for name in layer_names]

    # Create the feature extraction model
    down_stack = models.Model(inputs=base_model.input, outputs=layers)
    down_stack.trainable = False  # Freeze the layers to preserve pre-trained weights
    return down_stack

def upsample(filters, size, apply_dropout=False):
    initializer = tf.random_normal_initializer(0., 0.02)
    result = models.Sequential()
    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))
    result.add(layers.BatchNormalization())
    if apply_dropout:
        result.add(layers.Dropout(0.5))
    result.add(layers.ReLU())
    return result
def unet_model(output_channels, input_shape=(128, 128, 3)):
    inputs = layers.Input(shape=input_shape)
    encoder = create_mobilenet_encoder(input_shape)
    skips = encoder(inputs)
    x = skips[-1]
    skips = reversed(skips[:-1])

    # Upsampling and establishing the skip connections
    for up, skip in zip([512, 256, 128, 64], skips):
        x = upsample(up, 3)(x)
        concat = layers.Concatenate()
        x = concat([x, skip])

    # This is the last layer of the model
    last = layers.Conv2DTranspose(output_channels, 3, strides=2, padding='same')  #128x128 -> 256x256
    x = last(x)

    return models.Model(inputs=inputs, outputs=x)
from tensorflow.keras import backend as K
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Dice Coefficient
def dice_coef(y_true, y_pred, smooth=1):
    y_true_f = K.flatten(K.cast(y_true, 'float32'))
    y_pred_f = K.flatten(K.cast(y_pred, 'float32'))
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

# Dice with BCE Loss
def dice_p_bce(y_true, y_pred):
    # Binary Cross-Entropy loss
    bce_loss = binary_crossentropy(y_true, y_pred)
    
    # Dice coefficient loss
    dice_loss = 1 - dice_coef(y_true, y_pred)
    
    # Combined loss with a weighting factor for dice loss
    alpha = 0.5  # Adjust the alpha to change the weight of dice loss in the overall loss function
    combo_loss = bce_loss + alpha * dice_loss
    
    return combo_loss

# Model Compilation
# Ensure your model is named correctly, here it is assumed to be `mobilenetv2_unet`
mobilenetv2_unet = unet_model(output_channels=1)  # Adjust output_channels based on your needs, e.g., 1 for binary segmentation
mobilenetv2_unet.compile(optimizer=Adam(learning_rate=1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef])

# Model Summary (optional)
mobilenetv2_unet.summary()


from tensorflow.keras import backend as K
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Dice Coefficient
def dice_coef(y_true, y_pred, smooth=1e-6):
    y_true_f = K.flatten(K.cast(y_true, 'float32'))
    y_pred_f = K.flatten(K.cast(y_pred, 'float32'))
    intersection = K.sum(y_true_f * y_pred_f)
    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)
    
    if K.any(K.is_nan(dice)):
        print("NaN detected in dice coefficient")
        print(f"y_true: {y_true_f}")
        print(f"y_pred: {y_pred_f}")
        print(f"intersection: {intersection}")
        print(f"dice: {dice}")
    
    return dice

# Dice with BCE Loss
def dice_p_bce(y_true, y_pred):
    # Binary Cross-Entropy loss
    bce_loss = binary_crossentropy(y_true, y_pred)
    
    if K.any(K.is_nan(bce_loss)):
        print("NaN detected in BCE loss")
        print(f"y_true: {y_true}")
        print(f"y_pred: {y_pred}")
        print(f"bce_loss: {bce_loss}")
    
    # Dice coefficient loss
    dice_loss = 1 - dice_coef(y_true, y_pred)
    
    if K.any(K.is_nan(dice_loss)):
        print("NaN detected in Dice loss")
        print(f"y_true: {y_true}")
        print(f"y_pred: {y_pred}")
        print(f"dice_loss: {dice_loss}")
    
    # Combined loss with a weighting factor for dice loss
    alpha = 0.5  # Adjust the alpha to change the weight of dice loss in the overall loss function
    combo_loss = bce_loss + alpha * dice_loss
    
    if K.any(K.is_nan(combo_loss)):
        print("NaN detected in combined loss")
        print(f"bce_loss: {bce_loss}")
        print(f"dice_loss: {dice_loss}")
        print(f"combo_loss: {combo_loss}")
    
    return combo_loss

mobilenetv2_unet.compile(optimizer=Adam(learning_rate=1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef])
